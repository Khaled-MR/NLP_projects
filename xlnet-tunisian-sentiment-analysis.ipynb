{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hii everyone this is my first notebook please show your support and let me know if you have remarques or questions.\n",
    "Enjoy !!!! XD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I'll show you how to finetune the pretrained XLNet model with the huggingface library in order to achieve a sentiment analysis on Tunisian dataset already available on Kaggle.\n",
    "\n",
    "In fact, Hugging Face initially supported only PyTorch, but now TF 2.0 is also well supported.\n",
    "\n",
    "Following is a general pipeline for any transformer model:\n",
    "**Tokenizer definition →Tokenization of Documents →Model Definition →Model Training →Inference**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the link to the dataset https://www.kaggle.com/naim99/tunisian-texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tunisian-texts/tun.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt= pd.read_excel('../input/tunisian-texts/tun.xlsx',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>data_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[المجلس الأعلى للأمن الجزائر برئاسة الرئيس عبد...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[سنتخذ إجراءات لحماية حدودنا وإقليمنا وسنفعل د...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[الصين تطلق الصاروخ الفضائي القوي (longue marc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[المصدرfrance 24]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[هذا عام ينقصي أعمارنا وعام يقبل علينا فماذا ف...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32813</th>\n",
       "      <td>[علم موقع نسمة، باخرة إيطالية، وصلت ساعة متأخر...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32814</th>\n",
       "      <td>[رجع الهم، زايد بلاد بلا راجل، موش سكرتو الحدو...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32815</th>\n",
       "      <td>[نداء الى رئيس الجمهورية]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32816</th>\n",
       "      <td>[هبط الجيش واقفل الحدود وأعلن الحالة القصوى وك...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32817</th>\n",
       "      <td>[ملى لعب]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32818 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   texts  data_labels\n",
       "0      [المجلس الأعلى للأمن الجزائر برئاسة الرئيس عبد...            1\n",
       "1      [سنتخذ إجراءات لحماية حدودنا وإقليمنا وسنفعل د...            1\n",
       "2      [الصين تطلق الصاروخ الفضائي القوي (longue marc...            1\n",
       "3                                      [المصدرfrance 24]            1\n",
       "4      [هذا عام ينقصي أعمارنا وعام يقبل علينا فماذا ف...            1\n",
       "...                                                  ...          ...\n",
       "32813  [علم موقع نسمة، باخرة إيطالية، وصلت ساعة متأخر...            0\n",
       "32814  [رجع الهم، زايد بلاد بلا راجل، موش سكرتو الحدو...            0\n",
       "32815                          [نداء الى رئيس الجمهورية]            0\n",
       "32816  [هبط الجيش واقفل الحدود وأعلن الحالة القصوى وك...            0\n",
       "32817                                          [ملى لعب]            0\n",
       "\n",
       "[32818 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Text cleanning: **\n",
    "- remove strange characters\n",
    "- remove URLs (they doesn't tell us pretty much)\n",
    "- replace usernames for \"@\" character\n",
    "- remove line breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Remove emojis and special chars\n",
    "    clean=text\n",
    "    reg = re.compile('\\\\.+?(?=\\B|$)')\n",
    "    clean = text.apply(lambda r: re.sub(reg, string=r, repl=''))\n",
    "    reg = re.compile('\\x89Û_')\n",
    "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n",
    "    reg = re.compile('\\&amp')\n",
    "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='&'))\n",
    "    reg = re.compile('\\\\n')\n",
    "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n",
    "\n",
    "    #Remove hashtag symbol (#)\n",
    "    clean = clean.apply(lambda r: r.replace('#', ''))\n",
    "\n",
    "    #Remove user names\n",
    "    reg = re.compile('@[a-zA-Z0-9\\_]+')\n",
    "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='@'))\n",
    "\n",
    "    #Remove URLs\n",
    "    reg = re.compile('https?\\S+(?=\\s|$)')\n",
    "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='www'))\n",
    "\n",
    "    #Lowercase\n",
    "    clean = clean.apply(lambda r: r.lower())\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['clean'] = clean_text(dt['texts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer Definition**\n",
    "\n",
    "Every transformer based model has a unique tokenization technique, unique use of special tokens. The transformer library takes care of this for us. It supports tokenization for every model which is associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFXLNetModel, XLNetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fine-tune a Pretrained transformer model on custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlnet_model = 'xlnet-large-cased'\n",
    "xlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xlnet(mname):\n",
    "    \"\"\" Creates the model. It is composed of the XLNet main block and then\n",
    "    a classification head its added\n",
    "    \"\"\"\n",
    "    # Define token ids as inputs\n",
    "    word_inputs = tf.keras.Input(shape=(120,), name='word_inputs', dtype='int32')\n",
    "\n",
    "    # Call XLNet model\n",
    "    xlnet = TFXLNetModel.from_pretrained(mname)\n",
    "    xlnet_encodings = xlnet(word_inputs)[0]\n",
    "\n",
    "    # CLASSIFICATION HEAD \n",
    "    # Collect last step from last hidden state (CLS)\n",
    "    doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)\n",
    "    # Apply dropout for regularization\n",
    "    doc_encoding = tf.keras.layers.Dropout(.1)(doc_encoding)\n",
    "    # Final output \n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(doc_encoding)\n",
    "\n",
    "    # Compile model\n",
    "    model = tf.keras.Model(inputs=[word_inputs], outputs=[outputs])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f0741c1e128>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 71)\n"
     ]
    }
   ],
   "source": [
    "xlnet = create_xlnet(xlnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_inputs (InputLayer)     [(None, 120)]             0         \n",
      "_________________________________________________________________\n",
      "tfxl_net_model (TFXLNetModel ((None, 120, 1024),)      360268800 \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 1, 1024)]         0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorF [(None, 1024)]            0         \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "outputs (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 360,269,825\n",
      "Trainable params: 360,269,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "xlnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = dt['clean']\n",
    "labels = dt['data_labels']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.15, random_state=196)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is now to perform tokenization on documents. It can be performed either by encode() or encode_plus() method.\n",
    "\n",
    "XLNet requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n",
    "\n",
    "**input ids**: a sequence of integers identifying each input token to its index number in the XLNet tokenizer vocabulary\n",
    "\n",
    "**segment mask**: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long.(for two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence)\n",
    "\n",
    "**attention mask**: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens (we'll detail this in the next paragraph)\n",
    "\n",
    "**max_length**: Max length of any sentence to tokenize, its a hyperparameter. (originally BERT has 512 max length)\n",
    "\n",
    "**pad_to_max_length**: perform padding operation.\n",
    "\n",
    "![](http://)**add_special_tokens**: used to add special character like < cls >, < sep >,< unk >, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(tweets, tokenizer, max_len=120):\n",
    "    \"\"\" Gets tensors from text using the tokenizer provided\"\"\"\n",
    "    inps = [tokenizer.encode_plus(t, max_length=max_len, pad_to_max_length=True, add_special_tokens=True) for t in tweets]\n",
    "    inp_tok = np.array([a['input_ids'] for a in inps])\n",
    "    ids = np.array([a['attention_mask'] for a in inps])\n",
    "    segments = np.array([a['token_type_ids'] for a in inps])\n",
    "    return inp_tok, ids, segments\n",
    "\n",
    "def warmup(epoch, lr):\n",
    "    \"\"\"Used for increasing the learning rate slowly, this tends to achieve better convergence.\n",
    "    However, as we are finetuning for few epoch it's not crucial.\n",
    "    \"\"\"\n",
    "    return max(lr +1e-6, 2e-5)\n",
    "\n",
    "def plot_metrics(pred, true_labels):\n",
    "    \"\"\"Plots a ROC curve with the accuracy and the AUC\"\"\"\n",
    "    acc = accuracy_score(true_labels, np.array(pred.flatten() >= .5, dtype='int'))\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, pred)\n",
    "    auc = roc_auc_score(true_labels, pred)\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(8,8))\n",
    "    ax.plot(fpr, tpr, color='red')\n",
    "    ax.plot([0,1], [0,1], color='black', linestyle='--')\n",
    "    ax.set_title(f\"AUC: {auc}\\nACC: {acc}\");\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the input data (tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_tok, ids, segments = get_inputs(X_train, xlnet_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, min_delta=0.02, restore_best_weights=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(warmup, verbose=0),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=1e-6, patience=2, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=1e-6)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used one epoch but ideally you can try 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23710 samples, validate on 4185 samples\n",
      "23710/23710 [==============================] - 1507s 64ms/sample - loss: 0.4427 - accuracy: 0.8296 - precision: 0.8598 - recall: 0.9557 - val_loss: 0.3770 - val_accuracy: 0.8559 - val_precision: 0.8689 - val_recall: 0.9787\n"
     ]
    }
   ],
   "source": [
    "hist = xlnet.fit(x=inp_tok, y=y_train, epochs=1, batch_size=16, validation_split=.15, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 4s 4s/sample\n"
     ]
    }
   ],
   "source": [
    "text=['المنوج باهي برشا ومحلاه']\n",
    "inp_tok, ids, segments = get_inputs(text, xlnet_tokenizer)\n",
    "preds = xlnet.predict(inp_tok, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79322964]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 56ms/sample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.8066348]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=['كانت الخدمة super!']\n",
    "inp_tok, ids, segments = get_inputs(text, xlnet_tokenizer)\n",
    "preds = xlnet.predict(inp_tok, verbose=True)\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
