{"metadata":{"accelerator":"GPU","colab":{"name":"sentence_embeddings_with_sbert","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4550791,"sourceType":"datasetVersion","datasetId":2656775}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## RoBERTa and RoBERTa Siamese networks\n\nRoBERTa can be used for semantic textual similarity tasks, where two sentences\nare passed to the model and the network predicts whether they are similar or not. \n\nA common method to overcome the time overhead issue is to pass one sentence to the model,\nthen average the output of the model, or take the first token  and use\nthem as a sentence embedding, then use a vector similarity measure like cosine similarity or Manhatten / Euclidean distance\nto find close sentences (semantically similar sentences).\n\nIf we use RoBERTa directly, that will yield rather bad sentence embeddings. But if we\nfine-tune RoBERTa using a Siamese network, that will generate semantically meaningful\nsentence embeddings. This will enable RoBERTa to be used for new tasks. These tasks\ninclude:\n\n- Large-scale semantic similarity comparison.\n- Clustering.\n- Information retrieval via semantic search.\n\nIn this example, we will fine-tune a RoBERTa model using a Siamese network\nsuch that it will be able to produce semantically meaningful sentence embeddings and use\nthem in a semantic search and clustering example.\n","metadata":{"id":"BGZfBrMeeOLO"}},{"cell_type":"markdown","source":"## Installation\n\nWe will be using keras-nlp here","metadata":{"id":"SA6PwJ2DeOLQ"}},{"cell_type":"code","source":"!pip install keras-nlp -q","metadata":{"id":"OHIwHh5ieOLS","execution":{"iopub.status.busy":"2024-09-26T04:30:46.883804Z","iopub.execute_input":"2024-09-26T04:30:46.884229Z","iopub.status.idle":"2024-09-26T04:31:00.455444Z","shell.execute_reply.started":"2024-09-26T04:30:46.884194Z","shell.execute_reply":"2024-09-26T04:31:00.453958Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Enabling mixed precision training. This will help us reduce the training time.","metadata":{}},{"cell_type":"code","source":"import keras_nlp\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport sklearn.cluster as cluster\nfrom tensorflow import keras\n\npolicy = keras.mixed_precision.Policy(\"mixed_float16\")\nkeras.mixed_precision.set_global_policy(policy)","metadata":{"id":"XaEGHp6YeOLU","execution":{"iopub.status.busy":"2024-09-26T04:30:12.231281Z","iopub.execute_input":"2024-09-26T04:30:12.231674Z","iopub.status.idle":"2024-09-26T04:30:21.626517Z","shell.execute_reply.started":"2024-09-26T04:30:12.231641Z","shell.execute_reply":"2024-09-26T04:30:21.625163Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/__init__.py:29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Import everything from /api/ into keras.\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# Import * ignores names start with \"_\".\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Add everything in /api/ to the module search path.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/api/__init__.py:20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2024 The KerasNLP Authors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/api/layers/__init__.py:20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2024 The KerasNLP Authors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malibi_bias\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlibiBias\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcached_multi_head_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     CachedMultiHeadAttention,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mf_net_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FNetEncoder\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/layers/modeling/alibi_bias.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_nlp_export\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@keras_nlp_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_nlp.layers.AlibiBias\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAlibiBias\u001b[39;00m(keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLayer):\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'ops' from 'keras' (/opt/conda/lib/python3.10/site-packages/keras/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'ops' from 'keras' (/opt/conda/lib/python3.10/site-packages/keras/__init__.py)","output_type":"error"}]},{"cell_type":"markdown","source":"## Fine-tune the model using siamese networks\n\nSiamese network is a neural network architecture that contains two or more subnetworks. The subnetworks share the\nsame weights. It is used to generate feature vectors for each input and then compare them\nfor similarity.\n\nFor building the siamese network with the regression objective function, the siamese\nnetwork is asked to predict the cosine similarity between the embeddings of the two input\nsentences.\n\nCosine similarity indicates the angle between the sentence embeddings. If the cosine\nsimilarity is high, that means there is a small angle between the embeddings; hence, they\nare semantically similar.","metadata":{"id":"wwWBsm_feOLV"}},{"cell_type":"markdown","source":"### Fine-tune using the regression objective function\n\n","metadata":{"id":"wX7pXKMIeOLW"}},{"cell_type":"markdown","source":"#### Load the dataset\n\nWe will use the STSB dataset to fine-tune the model for the regression objective. STSB\nconsists of a collection of sentence pairs that are labelled in the range [0, 5]. 0\nindicates the least semantic similarity between the two sentences, and 5 indicates the\nmost semantic similarity between the two sentences.\n\nThe range of the cosine similarity is [-1, 1] and it's the output of the siamese network,\nbut the range of the labels in the dataset is [0, 5]. We need to unify the range between\nthe cosine similarity and the dataset labels, so while preparing the dataset, we will\ndivide the labels by 2.5 and subtract 1.","metadata":{"id":"iUqJV4p8eOLX"}},{"cell_type":"code","source":"TRAIN_BATCH_SIZE = 6\nVALIDATION_BATCH_SIZE = 8\n\nTRAIN_NUM_BATCHS = 300\nVALIDATION_NUM_BATCHS = 40\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n\ndef change_range(x):\n    return (x / 2.5) - 1\n\n\ndef prepare_dataset(dataset, num_batchs, batch_size):\n    dataset = dataset.map(\n        lambda z: (\n            [z[\"sentence1\"], z[\"sentence2\"]],\n            [tf.cast(change_range(z[\"label\"]), tf.float32)],\n        ),\n        num_parallel_calls=AUTOTUNE,\n    )\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.take(num_batchs)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\n\nstsb_ds = tfds.load(\n    \"glue/stsb\",\n)\nstsb_train, stsb_valid = stsb_ds[\"train\"], stsb_ds[\"validation\"]\n\nstsb_train = prepare_dataset(stsb_train, TRAIN_NUM_BATCHS, TRAIN_BATCH_SIZE)\nstsb_valid = prepare_dataset(stsb_valid, VALIDATION_NUM_BATCHS, VALIDATION_BATCH_SIZE)","metadata":{"id":"L4nhLkpIeOLY","execution":{"iopub.status.busy":"2023-09-05T08:53:32.998403Z","iopub.execute_input":"2023-09-05T08:53:32.999134Z","iopub.status.idle":"2023-09-05T08:53:39.608274Z","shell.execute_reply.started":"2023-09-05T08:53:32.999096Z","shell.execute_reply":"2023-09-05T08:53:39.607314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see examples from the dataset of two sentenses and their similarity.","metadata":{"id":"FkWlXn9aeOLY"}},{"cell_type":"code","source":"for x, y in stsb_train:\n    for i, example in enumerate(x):\n        print(f\"sentence 1 : {example[0]} \")\n        print(f\"sentence 2 : {example[1]} \")\n        print(f\"similarity : {y[i]} \\n\")\n    break","metadata":{"id":"85yS2EHWeOLZ","execution":{"iopub.status.busy":"2023-09-05T08:56:58.815397Z","iopub.execute_input":"2023-09-05T08:56:58.816083Z","iopub.status.idle":"2023-09-05T08:56:58.973961Z","shell.execute_reply.started":"2023-09-05T08:56:58.816042Z","shell.execute_reply":"2023-09-05T08:56:58.972937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoder model.\n\nEncoder model will produce the sentence embeddings. It consists\nof:\n\n- A preprocessor layer to tokenize and generate padding masks for the sentences.\n- A backbone model that will generate the contextual representation of each token in the\nsentence.\n- A mean pooling layer to produce the embeddings. We will use `keras.layers.GlobalAveragePooling1D`\nto apply the mean pooling to the backbone outputs. We will pass the padding mask to the\nlayer to exclude padded tokens from being averaged.\n- A normalization layer to normalize the embeddings as we are using the cosine similarity.","metadata":{"id":"jBQ9okKMeOLZ"}},{"cell_type":"code","source":"preprocessor = keras_nlp.models.RobertaPreprocessor.from_preset(\"roberta_base_en\")\nbackbone = keras_nlp.models.RobertaBackbone.from_preset(\"roberta_base_en\")\ninputs = keras.Input(shape=(1), dtype=\"string\", name=\"sentence\")\nx = preprocessor(inputs)\nh = backbone(x)\nembedding = keras.layers.GlobalAveragePooling1D(name=\"pooling_layer\")(\n    h, x[\"padding_mask\"]\n)\nn_embedding = tf.linalg.normalize(embedding, axis=1)[0]\nroberta_normal_encoder = keras.Model(inputs=inputs, outputs=n_embedding)\n\nroberta_normal_encoder.summary()","metadata":{"id":"a8wFM_gAeOLa","execution":{"iopub.status.busy":"2023-09-05T08:57:01.139928Z","iopub.execute_input":"2023-09-05T08:57:01.140285Z","iopub.status.idle":"2023-09-05T08:57:34.87103Z","shell.execute_reply.started":"2023-09-05T08:57:01.140253Z","shell.execute_reply":"2023-09-05T08:57:34.870031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Siamese network has two or more subnetworks, and for this Siamese model, we need two encoders. But we don't have two encoders; we have only one encoder, but we will pass the two sentences through it. That way, we can have two paths\nto get the embeddings and also shared weights between the two paths.\n\nAfter passing the two sentences to the model and getting the normalized embeddings, we will multiply the two normalized embeddings to get the cosine similarity between the two sentences.","metadata":{"id":"vhWNvI2jeOLa"}},{"cell_type":"code","source":"sentences = [\n    \"Today is a very sunny day.\",\n    \"I am hungry, I will get my meal.\",\n    \"The dog is eating his food.\",\n]\nquery = [\"The dog is enjoying his meal.\"]\n\nencoder = roberta_normal_encoder\n\nsentence_embeddings = encoder(tf.constant(sentences))\nquery_embedding = encoder(tf.constant(query))\n\ncosine_similarity_scores = tf.matmul(query_embedding, tf.transpose(sentence_embeddings))\nfor i, sim in enumerate(cosine_similarity_scores[0]):\n    print(f\"cosine similarity score between sentence {i+1} and the query = {sim} \")","metadata":{"id":"iviNd9uYeOLb","execution":{"iopub.status.busy":"2023-09-05T08:57:34.873066Z","iopub.execute_input":"2023-09-05T08:57:34.873873Z","iopub.status.idle":"2023-09-05T08:57:39.078649Z","shell.execute_reply.started":"2023-09-05T08:57:34.873835Z","shell.execute_reply":"2023-09-05T08:57:39.077579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the training we will use `MeanSquaredError()` as loss function, and `Adam()`\noptimizer with learning rate = 2e-5.","metadata":{"id":"8kQTJWKreOLb"}},{"cell_type":"code","source":"class RegressionSiamese(keras.Model):\n    def __init__(self, encoder, **kwargs):\n        inputs = keras.Input(shape=(2), dtype=\"string\", name=\"sentences\")\n        sen1, sen2 = tf.split(inputs, num_or_size_splits=2, axis=1, name=\"split\")\n        u = encoder(sen1)\n        v = encoder(sen2)\n        cosine_similarity_scores = tf.matmul(u, tf.transpose(v))\n\n        super().__init__(\n            inputs=inputs,\n            outputs=cosine_similarity_scores,\n            **kwargs,\n        )\n\n        self.encoder = encoder\n\n    def get_encoder(self):\n        return self.encoder","metadata":{"execution":{"iopub.status.busy":"2023-09-05T09:08:03.919764Z","iopub.execute_input":"2023-09-05T09:08:03.92014Z","iopub.status.idle":"2023-09-05T09:08:03.931319Z","shell.execute_reply.started":"2023-09-05T09:08:03.920109Z","shell.execute_reply":"2023-09-05T09:08:03.930218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_regression_siamese = RegressionSiamese(roberta_normal_encoder)\n\nroberta_regression_siamese.compile(\n    loss=keras.losses.MeanSquaredError(),\n    optimizer=keras.optimizers.Adam(2e-5),\n)\n\nroberta_regression_siamese.fit(stsb_train, validation_data=stsb_valid, epochs=1)","metadata":{"id":"4Nh8Zi1VeOLb","execution":{"iopub.status.busy":"2023-09-05T09:08:05.446125Z","iopub.execute_input":"2023-09-05T09:08:05.446483Z","iopub.status.idle":"2023-09-05T09:14:48.259649Z","shell.execute_reply.started":"2023-09-05T09:08:05.446451Z","shell.execute_reply":"2023-09-05T09:14:48.258492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try the model after training, we will notice a huge difference in the output. That\nmeans that the model after fine-tuning is capable of producing semantically meaningful\nembeddings. where the semantically similar sentences have a small angle between them. and\nsemantically dissimilar sentences have a large angle between them.","metadata":{"id":"hIow18mOeOLc"}},{"cell_type":"code","source":"sentences = [\n    \"Today is a very sunny day.\",\n    \"I am hungry, I will get my meal.\",\n    \"The dog is eating his food.\",\n]\nquery = [\"The dog is enjoying his food.\"]\n\nencoder = roberta_regression_siamese.get_encoder()\n\nsentence_embeddings = encoder(tf.constant(sentences))\nquery_embedding = encoder(tf.constant(query))\n\ncosine_simalarities = tf.matmul(query_embedding, tf.transpose(sentence_embeddings))\nfor i, sim in enumerate(cosine_simalarities[0]):\n    print(f\"cosine similarity between sentence {i+1} and the query = {sim} \")","metadata":{"id":"0q9hsDHQeOLc","execution":{"iopub.status.busy":"2023-09-05T09:14:48.261723Z","iopub.execute_input":"2023-09-05T09:14:48.262118Z","iopub.status.idle":"2023-09-05T09:14:49.352532Z","shell.execute_reply.started":"2023-09-05T09:14:48.26204Z","shell.execute_reply":"2023-09-05T09:14:49.351557Z"},"trusted":true},"execution_count":null,"outputs":[]}]}