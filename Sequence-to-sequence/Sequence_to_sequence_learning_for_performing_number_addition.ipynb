{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fy91b1PX4KtX"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# Parameters for the model and dataset.\n",
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "REVERSE = True\n",
        "\n",
        "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
        "# int is DIGITS.\n",
        "MAXLEN = DIGITS + 1 + DIGITS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharacterTable:\n",
        "    \"\"\"Given a set of characters:\n",
        "    + Encode them to a one-hot integer representation\n",
        "    + Decode the one-hot or integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"Initialize character table.\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One-hot encode given string C.\n",
        "        # Arguments\n",
        "            C: string, to be encoded.\n",
        "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"Decode the given vector or 2D array to their character output.\n",
        "        # Arguments\n",
        "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
        "                or a vector of character indices (used with `calc_argmax=False`).\n",
        "            calc_argmax: Whether to find the character index with maximum\n",
        "                probability, defaults to `True`.\n",
        "        \"\"\"\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return \"\".join(self.indices_char[x] for x in x)\n",
        "\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = \"0123456789+ \"\n",
        "ctable = CharacterTable(chars)\n",
        "\n",
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print(\"Generating data...\")\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(\n",
        "        \"\".join(\n",
        "            np.random.choice(list(\"0123456789\"))\n",
        "            for i in range(np.random.randint(1, DIGITS + 1))\n",
        "        )\n",
        "    )\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = \"{}+{}\".format(a, b)\n",
        "    query = q + \" \" * (MAXLEN - len(q))\n",
        "    ans = str(a + b)\n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
        "    if REVERSE:\n",
        "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
        "        # space used for padding.)\n",
        "        query = query[::-1]\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "print(\"Total questions:\", len(questions))"
      ],
      "metadata": {
        "id": "yqxLcRyh4Sze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c0e9c5d-dfaa-4985-f681-9e45eed0cb1d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating data...\n",
            "Total questions: 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vectorization...\")\n",
        "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=bool)\n",
        "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=bool)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "\n",
        "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
        "# digits.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Explicitly set apart 10% for validation data that we never train over.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print(\"Training Data:\")\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(\"Validation Data:\")\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "id": "D0Xm4rKL4auP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13337bf9-efc9-4cf5-de1b-7c5225aab7f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorization...\n",
            "Training Data:\n",
            "(45000, 7, 12)\n",
            "(45000, 4, 12)\n",
            "Validation Data:\n",
            "(5000, 7, 12)\n",
            "(5000, 4, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Build model...\")\n",
        "num_layers = 1  # Try to add more LSTM layers!\n",
        "\n",
        "model = keras.Sequential()\n",
        "# \"Encode\" the input sequence using a LSTM, producing an output of size 128.\n",
        "# Note: In a situation where your input sequences have a variable length,\n",
        "# use input_shape=(None, num_feature).\n",
        "model.add(layers.Input((MAXLEN, len(chars))))\n",
        "model.add(layers.LSTM(128))\n",
        "# As the decoder RNN's input, repeatedly provide with the last output of\n",
        "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
        "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
        "model.add(layers.RepeatVector(DIGITS + 1))\n",
        "# The decoder RNN could be multiple layers stacked or a single layer.\n",
        "for _ in range(num_layers):\n",
        "    # By setting return_sequences to True, return not only the last output but\n",
        "    # all the outputs so far in the form of (num_samples, timesteps,\n",
        "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
        "    # the first dimension to be the timesteps.\n",
        "    model.add(layers.LSTM(128, return_sequences=True))\n",
        "\n",
        "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
        "# of the output sequence, decide which character should be chosen.\n",
        "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "OFcXQ2Q04aq3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "e3ec9184-8242-4900-a270-bb5dec69cf84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m72,192\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ repeat_vector (\u001b[38;5;33mRepeatVector\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │         \u001b[38;5;34m131,584\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m12\u001b[0m)               │           \u001b[38;5;34m1,548\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">72,192</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ repeat_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,548</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m205,324\u001b[0m (802.05 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">205,324</span> (802.05 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m205,324\u001b[0m (802.05 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">205,324</span> (802.05 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters.\n",
        "epochs = 30\n",
        "batch_size = 32\n",
        "\n",
        "# Formatting characters for results display.\n",
        "green_color = \"\\033[92m\"\n",
        "red_color = \"\\033[91m\"\n",
        "end_char = \"\\033[0m\"\n",
        "\n",
        "# Train the model each generation and show predictions against the validation\n",
        "# dataset.\n",
        "for epoch in range(1, epochs):\n",
        "    print()\n",
        "    print(\"Iteration\", epoch)\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=1,\n",
        "        validation_data=(x_val, y_val),\n",
        "    )\n",
        "    # Select 10 samples from the validation set at random so we can visualize\n",
        "    # errors.\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        preds = np.argmax(model.predict(rowx, verbose=0), axis=-1)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
        "        print(\"T\", correct, end=\" \")\n",
        "        if correct == guess:\n",
        "            print(f\"{green_color}☑ {guess}{end_char}\")\n",
        "        else:\n",
        "            print(f\"{red_color}☒ {guess}{end_char}\")"
      ],
      "metadata": {
        "id": "ih9HaghM4anW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d05483-8ff4-4fde-c6bd-15d15fee2e87"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 1\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.3241 - loss: 1.8780 - val_accuracy: 0.4180 - val_loss: 1.5570\n",
            "Q 5+791   T 796  \u001b[91m☒ 886 \u001b[0m\n",
            "Q 986+877 T 1863 \u001b[91m☒ 1511\u001b[0m\n",
            "Q 6+201   T 207  \u001b[91m☒ 122 \u001b[0m\n",
            "Q 738+5   T 743  \u001b[91m☒ 886 \u001b[0m\n",
            "Q 610+933 T 1543 \u001b[91m☒ 1496\u001b[0m\n",
            "Q 938+6   T 944  \u001b[91m☒ 990 \u001b[0m\n",
            "Q 189+55  T 244  \u001b[91m☒ 292 \u001b[0m\n",
            "Q 504+49  T 553  \u001b[91m☒ 566 \u001b[0m\n",
            "Q 271+501 T 772  \u001b[91m☒ 802 \u001b[0m\n",
            "Q 980+543 T 1523 \u001b[91m☒ 1496\u001b[0m\n",
            "\n",
            "Iteration 2\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.4541 - loss: 1.4587 - val_accuracy: 0.5723 - val_loss: 1.1633\n",
            "Q 49+687  T 736  \u001b[91m☒ 766 \u001b[0m\n",
            "Q 305+4   T 309  \u001b[91m☒ 310 \u001b[0m\n",
            "Q 609+777 T 1386 \u001b[91m☒ 1366\u001b[0m\n",
            "Q 99+977  T 1076 \u001b[91m☒ 1066\u001b[0m\n",
            "Q 847+65  T 912  \u001b[91m☒ 906 \u001b[0m\n",
            "Q 782+46  T 828  \u001b[91m☒ 806 \u001b[0m\n",
            "Q 58+49   T 107  \u001b[91m☒ 106 \u001b[0m\n",
            "Q 353+7   T 360  \u001b[91m☒ 366 \u001b[0m\n",
            "Q 548+598 T 1146 \u001b[91m☒ 1166\u001b[0m\n",
            "Q 814+5   T 819  \u001b[91m☒ 816 \u001b[0m\n",
            "\n",
            "Iteration 3\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.5903 - loss: 1.0996 - val_accuracy: 0.6543 - val_loss: 0.9271\n",
            "Q 548+27  T 575  \u001b[91m☒ 573 \u001b[0m\n",
            "Q 954+33  T 987  \u001b[91m☒ 990 \u001b[0m\n",
            "Q 4+229   T 233  \u001b[91m☒ 236 \u001b[0m\n",
            "Q 569+219 T 788  \u001b[91m☒ 787 \u001b[0m\n",
            "Q 571+55  T 626  \u001b[91m☒ 627 \u001b[0m\n",
            "Q 337+548 T 885  \u001b[91m☒ 990 \u001b[0m\n",
            "Q 422+627 T 1049 \u001b[91m☒ 1067\u001b[0m\n",
            "Q 982+777 T 1759 \u001b[91m☒ 1836\u001b[0m\n",
            "Q 196+48  T 244  \u001b[91m☒ 239 \u001b[0m\n",
            "Q 23+180  T 203  \u001b[91m☒ 207 \u001b[0m\n",
            "\n",
            "Iteration 4\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.6676 - loss: 0.8977 - val_accuracy: 0.7028 - val_loss: 0.8036\n",
            "Q 743+36  T 779  \u001b[91m☒ 786 \u001b[0m\n",
            "Q 670+379 T 1049 \u001b[91m☒ 1045\u001b[0m\n",
            "Q 66+408  T 474  \u001b[91m☒ 476 \u001b[0m\n",
            "Q 37+86   T 123  \u001b[91m☒ 126 \u001b[0m\n",
            "Q 600+60  T 660  \u001b[91m☒ 661 \u001b[0m\n",
            "Q 97+427  T 524  \u001b[91m☒ 526 \u001b[0m\n",
            "Q 797+8   T 805  \u001b[91m☒ 804 \u001b[0m\n",
            "Q 60+342  T 402  \u001b[91m☒ 406 \u001b[0m\n",
            "Q 928+6   T 934  \u001b[91m☒ 931 \u001b[0m\n",
            "Q 96+937  T 1033 \u001b[91m☒ 1036\u001b[0m\n",
            "\n",
            "Iteration 5\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7138 - loss: 0.7815 - val_accuracy: 0.7322 - val_loss: 0.7387\n",
            "Q 85+264  T 349  \u001b[91m☒ 343 \u001b[0m\n",
            "Q 9+785   T 794  \u001b[91m☒ 798 \u001b[0m\n",
            "Q 80+575  T 655  \u001b[91m☒ 653 \u001b[0m\n",
            "Q 310+740 T 1050 \u001b[91m☒ 1052\u001b[0m\n",
            "Q 766+265 T 1031 \u001b[91m☒ 1022\u001b[0m\n",
            "Q 308+86  T 394  \u001b[91m☒ 397 \u001b[0m\n",
            "Q 28+73   T 101  \u001b[91m☒ 90  \u001b[0m\n",
            "Q 530+3   T 533  \u001b[92m☑ 533 \u001b[0m\n",
            "Q 735+3   T 738  \u001b[92m☑ 738 \u001b[0m\n",
            "Q 295+944 T 1239 \u001b[91m☒ 1237\u001b[0m\n",
            "\n",
            "Iteration 6\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7428 - loss: 0.7048 - val_accuracy: 0.7542 - val_loss: 0.6602\n",
            "Q 751+67  T 818  \u001b[91m☒ 811 \u001b[0m\n",
            "Q 35+740  T 775  \u001b[92m☑ 775 \u001b[0m\n",
            "Q 55+929  T 984  \u001b[91m☒ 981 \u001b[0m\n",
            "Q 9+592   T 601  \u001b[91m☒ 690 \u001b[0m\n",
            "Q 4+449   T 453  \u001b[91m☒ 452 \u001b[0m\n",
            "Q 252+24  T 276  \u001b[91m☒ 275 \u001b[0m\n",
            "Q 633+191 T 824  \u001b[91m☒ 822 \u001b[0m\n",
            "Q 59+744  T 803  \u001b[91m☒ 801 \u001b[0m\n",
            "Q 746+25  T 771  \u001b[91m☒ 772 \u001b[0m\n",
            "Q 893+54  T 947  \u001b[91m☒ 941 \u001b[0m\n",
            "\n",
            "Iteration 7\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.7714 - loss: 0.6288 - val_accuracy: 0.8224 - val_loss: 0.4767\n",
            "Q 823+1   T 824  \u001b[92m☑ 824 \u001b[0m\n",
            "Q 853+507 T 1360 \u001b[91m☒ 1369\u001b[0m\n",
            "Q 49+274  T 323  \u001b[91m☒ 324 \u001b[0m\n",
            "Q 35+932  T 967  \u001b[91m☒ 966 \u001b[0m\n",
            "Q 596+10  T 606  \u001b[91m☒ 608 \u001b[0m\n",
            "Q 548+27  T 575  \u001b[91m☒ 574 \u001b[0m\n",
            "Q 773+27  T 800  \u001b[91m☒ 801 \u001b[0m\n",
            "Q 266+845 T 1111 \u001b[91m☒ 1110\u001b[0m\n",
            "Q 34+35   T 69   \u001b[91m☒ 78  \u001b[0m\n",
            "Q 330+27  T 357  \u001b[91m☒ 358 \u001b[0m\n",
            "\n",
            "Iteration 8\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.8581 - loss: 0.4015 - val_accuracy: 0.9421 - val_loss: 0.2231\n",
            "Q 444+2   T 446  \u001b[92m☑ 446 \u001b[0m\n",
            "Q 438+651 T 1089 \u001b[91m☒ 1099\u001b[0m\n",
            "Q 46+21   T 67   \u001b[92m☑ 67  \u001b[0m\n",
            "Q 90+631  T 721  \u001b[92m☑ 721 \u001b[0m\n",
            "Q 0+375   T 375  \u001b[92m☑ 375 \u001b[0m\n",
            "Q 790+7   T 797  \u001b[91m☒ 796 \u001b[0m\n",
            "Q 814+5   T 819  \u001b[92m☑ 819 \u001b[0m\n",
            "Q 349+80  T 429  \u001b[92m☑ 429 \u001b[0m\n",
            "Q 57+49   T 106  \u001b[91m☒ 10  \u001b[0m\n",
            "Q 64+116  T 180  \u001b[91m☒ 170 \u001b[0m\n",
            "\n",
            "Iteration 9\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9530 - loss: 0.1932 - val_accuracy: 0.9762 - val_loss: 0.1196\n",
            "Q 41+789  T 830  \u001b[92m☑ 830 \u001b[0m\n",
            "Q 25+845  T 870  \u001b[92m☑ 870 \u001b[0m\n",
            "Q 707+813 T 1520 \u001b[91m☒ 1510\u001b[0m\n",
            "Q 578+18  T 596  \u001b[92m☑ 596 \u001b[0m\n",
            "Q 661+76  T 737  \u001b[92m☑ 737 \u001b[0m\n",
            "Q 505+89  T 594  \u001b[92m☑ 594 \u001b[0m\n",
            "Q 472+94  T 566  \u001b[92m☑ 566 \u001b[0m\n",
            "Q 602+549 T 1151 \u001b[92m☑ 1151\u001b[0m\n",
            "Q 36+757  T 793  \u001b[92m☑ 793 \u001b[0m\n",
            "Q 536+9   T 545  \u001b[92m☑ 545 \u001b[0m\n",
            "\n",
            "Iteration 10\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9786 - loss: 0.1043 - val_accuracy: 0.9853 - val_loss: 0.0710\n",
            "Q 511+692 T 1203 \u001b[92m☑ 1203\u001b[0m\n",
            "Q 991+72  T 1063 \u001b[92m☑ 1063\u001b[0m\n",
            "Q 322+396 T 718  \u001b[92m☑ 718 \u001b[0m\n",
            "Q 52+176  T 228  \u001b[92m☑ 228 \u001b[0m\n",
            "Q 16+522  T 538  \u001b[92m☑ 538 \u001b[0m\n",
            "Q 430+78  T 508  \u001b[92m☑ 508 \u001b[0m\n",
            "Q 573+14  T 587  \u001b[92m☑ 587 \u001b[0m\n",
            "Q 837+72  T 909  \u001b[92m☑ 909 \u001b[0m\n",
            "Q 59+744  T 803  \u001b[92m☑ 803 \u001b[0m\n",
            "Q 24+665  T 689  \u001b[92m☑ 689 \u001b[0m\n",
            "\n",
            "Iteration 11\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9850 - loss: 0.0689 - val_accuracy: 0.9750 - val_loss: 0.0822\n",
            "Q 490+81  T 571  \u001b[92m☑ 571 \u001b[0m\n",
            "Q 806+917 T 1723 \u001b[92m☑ 1723\u001b[0m\n",
            "Q 487+72  T 559  \u001b[92m☑ 559 \u001b[0m\n",
            "Q 5+731   T 736  \u001b[92m☑ 736 \u001b[0m\n",
            "Q 939+774 T 1713 \u001b[91m☒ 1703\u001b[0m\n",
            "Q 6+502   T 508  \u001b[92m☑ 508 \u001b[0m\n",
            "Q 1+604   T 605  \u001b[91m☒ 606 \u001b[0m\n",
            "Q 910+62  T 972  \u001b[92m☑ 972 \u001b[0m\n",
            "Q 887+115 T 1002 \u001b[92m☑ 1002\u001b[0m\n",
            "Q 835+510 T 1345 \u001b[92m☑ 1345\u001b[0m\n",
            "\n",
            "Iteration 12\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9805 - loss: 0.0723 - val_accuracy: 0.9610 - val_loss: 0.1092\n",
            "Q 869+77  T 946  \u001b[91m☒ 956 \u001b[0m\n",
            "Q 192+61  T 253  \u001b[92m☑ 253 \u001b[0m\n",
            "Q 85+73   T 158  \u001b[92m☑ 158 \u001b[0m\n",
            "Q 4+928   T 932  \u001b[92m☑ 932 \u001b[0m\n",
            "Q 515+50  T 565  \u001b[92m☑ 565 \u001b[0m\n",
            "Q 184+101 T 285  \u001b[92m☑ 285 \u001b[0m\n",
            "Q 355+53  T 408  \u001b[92m☑ 408 \u001b[0m\n",
            "Q 842+617 T 1459 \u001b[92m☑ 1459\u001b[0m\n",
            "Q 95+629  T 724  \u001b[92m☑ 724 \u001b[0m\n",
            "Q 824+803 T 1627 \u001b[92m☑ 1627\u001b[0m\n",
            "\n",
            "Iteration 13\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9922 - loss: 0.0363 - val_accuracy: 0.9966 - val_loss: 0.0210\n",
            "Q 83+65   T 148  \u001b[92m☑ 148 \u001b[0m\n",
            "Q 12+766  T 778  \u001b[92m☑ 778 \u001b[0m\n",
            "Q 689+869 T 1558 \u001b[92m☑ 1558\u001b[0m\n",
            "Q 171+523 T 694  \u001b[92m☑ 694 \u001b[0m\n",
            "Q 78+431  T 509  \u001b[92m☑ 509 \u001b[0m\n",
            "Q 378+86  T 464  \u001b[92m☑ 464 \u001b[0m\n",
            "Q 70+680  T 750  \u001b[92m☑ 750 \u001b[0m\n",
            "Q 627+721 T 1348 \u001b[92m☑ 1348\u001b[0m\n",
            "Q 305+4   T 309  \u001b[92m☑ 309 \u001b[0m\n",
            "Q 613+840 T 1453 \u001b[92m☑ 1453\u001b[0m\n",
            "\n",
            "Iteration 14\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9890 - loss: 0.0418 - val_accuracy: 0.9973 - val_loss: 0.0181\n",
            "Q 938+59  T 997  \u001b[92m☑ 997 \u001b[0m\n",
            "Q 865+887 T 1752 \u001b[92m☑ 1752\u001b[0m\n",
            "Q 62+533  T 595  \u001b[92m☑ 595 \u001b[0m\n",
            "Q 25+635  T 660  \u001b[91m☒ 650 \u001b[0m\n",
            "Q 254+131 T 385  \u001b[92m☑ 385 \u001b[0m\n",
            "Q 308+675 T 983  \u001b[92m☑ 983 \u001b[0m\n",
            "Q 73+454  T 527  \u001b[92m☑ 527 \u001b[0m\n",
            "Q 76+19   T 95   \u001b[92m☑ 95  \u001b[0m\n",
            "Q 64+47   T 111  \u001b[92m☑ 111 \u001b[0m\n",
            "Q 33+254  T 287  \u001b[92m☑ 287 \u001b[0m\n",
            "\n",
            "Iteration 15\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9959 - loss: 0.0206 - val_accuracy: 0.9979 - val_loss: 0.0142\n",
            "Q 38+627  T 665  \u001b[92m☑ 665 \u001b[0m\n",
            "Q 946+1   T 947  \u001b[92m☑ 947 \u001b[0m\n",
            "Q 59+616  T 675  \u001b[92m☑ 675 \u001b[0m\n",
            "Q 349+9   T 358  \u001b[92m☑ 358 \u001b[0m\n",
            "Q 34+962  T 996  \u001b[92m☑ 996 \u001b[0m\n",
            "Q 281+892 T 1173 \u001b[92m☑ 1173\u001b[0m\n",
            "Q 76+75   T 151  \u001b[92m☑ 151 \u001b[0m\n",
            "Q 327+13  T 340  \u001b[92m☑ 340 \u001b[0m\n",
            "Q 44+334  T 378  \u001b[92m☑ 378 \u001b[0m\n",
            "Q 82+704  T 786  \u001b[92m☑ 786 \u001b[0m\n",
            "\n",
            "Iteration 16\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9947 - loss: 0.0242 - val_accuracy: 0.9980 - val_loss: 0.0128\n",
            "Q 272+24  T 296  \u001b[92m☑ 296 \u001b[0m\n",
            "Q 848+2   T 850  \u001b[92m☑ 850 \u001b[0m\n",
            "Q 862+987 T 1849 \u001b[92m☑ 1849\u001b[0m\n",
            "Q 578+3   T 581  \u001b[92m☑ 581 \u001b[0m\n",
            "Q 498+794 T 1292 \u001b[92m☑ 1292\u001b[0m\n",
            "Q 473+9   T 482  \u001b[92m☑ 482 \u001b[0m\n",
            "Q 5+267   T 272  \u001b[92m☑ 272 \u001b[0m\n",
            "Q 758+8   T 766  \u001b[92m☑ 766 \u001b[0m\n",
            "Q 504+27  T 531  \u001b[92m☑ 531 \u001b[0m\n",
            "Q 960+13  T 973  \u001b[92m☑ 973 \u001b[0m\n",
            "\n",
            "Iteration 17\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9957 - loss: 0.0184 - val_accuracy: 0.9888 - val_loss: 0.0513\n",
            "Q 37+243  T 280  \u001b[92m☑ 280 \u001b[0m\n",
            "Q 59+276  T 335  \u001b[92m☑ 335 \u001b[0m\n",
            "Q 0+26    T 26   \u001b[92m☑ 26  \u001b[0m\n",
            "Q 499+260 T 759  \u001b[92m☑ 759 \u001b[0m\n",
            "Q 492+78  T 570  \u001b[92m☑ 570 \u001b[0m\n",
            "Q 31+247  T 278  \u001b[92m☑ 278 \u001b[0m\n",
            "Q 55+929  T 984  \u001b[92m☑ 984 \u001b[0m\n",
            "Q 952+305 T 1257 \u001b[92m☑ 1257\u001b[0m\n",
            "Q 496+47  T 543  \u001b[92m☑ 543 \u001b[0m\n",
            "Q 180+45  T 225  \u001b[92m☑ 225 \u001b[0m\n",
            "\n",
            "Iteration 18\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9964 - loss: 0.0170 - val_accuracy: 0.9931 - val_loss: 0.0232\n",
            "Q 3+14    T 17   \u001b[92m☑ 17  \u001b[0m\n",
            "Q 111+54  T 165  \u001b[92m☑ 165 \u001b[0m\n",
            "Q 78+98   T 176  \u001b[92m☑ 176 \u001b[0m\n",
            "Q 4+928   T 932  \u001b[92m☑ 932 \u001b[0m\n",
            "Q 783+92  T 875  \u001b[92m☑ 875 \u001b[0m\n",
            "Q 670+847 T 1517 \u001b[92m☑ 1517\u001b[0m\n",
            "Q 88+88   T 176  \u001b[92m☑ 176 \u001b[0m\n",
            "Q 43+447  T 490  \u001b[92m☑ 490 \u001b[0m\n",
            "Q 125+4   T 129  \u001b[92m☑ 129 \u001b[0m\n",
            "Q 566+96  T 662  \u001b[92m☑ 662 \u001b[0m\n",
            "\n",
            "Iteration 19\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9958 - loss: 0.0184 - val_accuracy: 0.9768 - val_loss: 0.1010\n",
            "Q 247+912 T 1159 \u001b[92m☑ 1159\u001b[0m\n",
            "Q 363+5   T 368  \u001b[92m☑ 368 \u001b[0m\n",
            "Q 994+60  T 1054 \u001b[92m☑ 1054\u001b[0m\n",
            "Q 608+76  T 684  \u001b[92m☑ 684 \u001b[0m\n",
            "Q 445+23  T 468  \u001b[92m☑ 468 \u001b[0m\n",
            "Q 193+99  T 292  \u001b[92m☑ 292 \u001b[0m\n",
            "Q 530+29  T 559  \u001b[92m☑ 559 \u001b[0m\n",
            "Q 6+69    T 75   \u001b[91m☒ 76  \u001b[0m\n",
            "Q 82+971  T 1053 \u001b[92m☑ 1053\u001b[0m\n",
            "Q 23+673  T 696  \u001b[92m☑ 696 \u001b[0m\n",
            "\n",
            "Iteration 20\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9935 - loss: 0.0262 - val_accuracy: 0.9624 - val_loss: 0.1180\n",
            "Q 2+860   T 862  \u001b[92m☑ 862 \u001b[0m\n",
            "Q 577+14  T 591  \u001b[92m☑ 591 \u001b[0m\n",
            "Q 76+735  T 811  \u001b[92m☑ 811 \u001b[0m\n",
            "Q 633+400 T 1033 \u001b[91m☒ 1034\u001b[0m\n",
            "Q 120+716 T 836  \u001b[91m☒ 837 \u001b[0m\n",
            "Q 122+153 T 275  \u001b[92m☑ 275 \u001b[0m\n",
            "Q 11+765  T 776  \u001b[92m☑ 776 \u001b[0m\n",
            "Q 4+180   T 184  \u001b[92m☑ 184 \u001b[0m\n",
            "Q 73+361  T 434  \u001b[92m☑ 434 \u001b[0m\n",
            "Q 132+88  T 220  \u001b[92m☑ 220 \u001b[0m\n",
            "\n",
            "Iteration 21\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9887 - loss: 0.0390 - val_accuracy: 0.9994 - val_loss: 0.0046\n",
            "Q 407+31  T 438  \u001b[92m☑ 438 \u001b[0m\n",
            "Q 924+19  T 943  \u001b[92m☑ 943 \u001b[0m\n",
            "Q 561+75  T 636  \u001b[92m☑ 636 \u001b[0m\n",
            "Q 46+980  T 1026 \u001b[92m☑ 1026\u001b[0m\n",
            "Q 22+61   T 83   \u001b[92m☑ 83  \u001b[0m\n",
            "Q 153+64  T 217  \u001b[92m☑ 217 \u001b[0m\n",
            "Q 248+24  T 272  \u001b[92m☑ 272 \u001b[0m\n",
            "Q 558+20  T 578  \u001b[92m☑ 578 \u001b[0m\n",
            "Q 45+921  T 966  \u001b[92m☑ 966 \u001b[0m\n",
            "Q 3+650   T 653  \u001b[92m☑ 653 \u001b[0m\n",
            "\n",
            "Iteration 22\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9901 - loss: 0.0340 - val_accuracy: 0.9948 - val_loss: 0.0209\n",
            "Q 63+833  T 896  \u001b[92m☑ 896 \u001b[0m\n",
            "Q 64+190  T 254  \u001b[92m☑ 254 \u001b[0m\n",
            "Q 38+389  T 427  \u001b[92m☑ 427 \u001b[0m\n",
            "Q 23+199  T 222  \u001b[91m☒ 223 \u001b[0m\n",
            "Q 65+49   T 114  \u001b[92m☑ 114 \u001b[0m\n",
            "Q 561+498 T 1059 \u001b[92m☑ 1059\u001b[0m\n",
            "Q 911+56  T 967  \u001b[92m☑ 967 \u001b[0m\n",
            "Q 94+928  T 1022 \u001b[92m☑ 1022\u001b[0m\n",
            "Q 754+15  T 769  \u001b[92m☑ 769 \u001b[0m\n",
            "Q 365+662 T 1027 \u001b[92m☑ 1027\u001b[0m\n",
            "\n",
            "Iteration 23\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9967 - loss: 0.0130 - val_accuracy: 0.9993 - val_loss: 0.0054\n",
            "Q 965+60  T 1025 \u001b[92m☑ 1025\u001b[0m\n",
            "Q 917+74  T 991  \u001b[92m☑ 991 \u001b[0m\n",
            "Q 373+840 T 1213 \u001b[92m☑ 1213\u001b[0m\n",
            "Q 90+718  T 808  \u001b[92m☑ 808 \u001b[0m\n",
            "Q 4+989   T 993  \u001b[92m☑ 993 \u001b[0m\n",
            "Q 530+29  T 559  \u001b[92m☑ 559 \u001b[0m\n",
            "Q 66+92   T 158  \u001b[92m☑ 158 \u001b[0m\n",
            "Q 228+885 T 1113 \u001b[92m☑ 1113\u001b[0m\n",
            "Q 977+967 T 1944 \u001b[92m☑ 1944\u001b[0m\n",
            "Q 4+238   T 242  \u001b[92m☑ 242 \u001b[0m\n",
            "\n",
            "Iteration 24\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9977 - loss: 0.0095 - val_accuracy: 0.9977 - val_loss: 0.0108\n",
            "Q 852+6   T 858  \u001b[92m☑ 858 \u001b[0m\n",
            "Q 835+633 T 1468 \u001b[92m☑ 1468\u001b[0m\n",
            "Q 950+6   T 956  \u001b[92m☑ 956 \u001b[0m\n",
            "Q 730+0   T 730  \u001b[92m☑ 730 \u001b[0m\n",
            "Q 877+743 T 1620 \u001b[92m☑ 1620\u001b[0m\n",
            "Q 43+255  T 298  \u001b[92m☑ 298 \u001b[0m\n",
            "Q 68+42   T 110  \u001b[92m☑ 110 \u001b[0m\n",
            "Q 52+68   T 120  \u001b[92m☑ 120 \u001b[0m\n",
            "Q 88+105  T 193  \u001b[92m☑ 193 \u001b[0m\n",
            "Q 44+606  T 650  \u001b[92m☑ 650 \u001b[0m\n",
            "\n",
            "Iteration 25\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9990 - loss: 0.0060 - val_accuracy: 0.9404 - val_loss: 0.2058\n",
            "Q 62+358  T 420  \u001b[92m☑ 420 \u001b[0m\n",
            "Q 1+276   T 277  \u001b[92m☑ 277 \u001b[0m\n",
            "Q 196+48  T 244  \u001b[92m☑ 244 \u001b[0m\n",
            "Q 72+9    T 81   \u001b[92m☑ 81  \u001b[0m\n",
            "Q 863+526 T 1389 \u001b[92m☑ 1389\u001b[0m\n",
            "Q 689+112 T 801  \u001b[91m☒ 701 \u001b[0m\n",
            "Q 257+641 T 898  \u001b[92m☑ 898 \u001b[0m\n",
            "Q 699+52  T 751  \u001b[92m☑ 751 \u001b[0m\n",
            "Q 5+855   T 860  \u001b[92m☑ 860 \u001b[0m\n",
            "Q 71+139  T 210  \u001b[92m☑ 210 \u001b[0m\n",
            "\n",
            "Iteration 26\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9853 - loss: 0.0483 - val_accuracy: 0.9974 - val_loss: 0.0118\n",
            "Q 36+710  T 746  \u001b[92m☑ 746 \u001b[0m\n",
            "Q 969+73  T 1042 \u001b[92m☑ 1042\u001b[0m\n",
            "Q 269+6   T 275  \u001b[92m☑ 275 \u001b[0m\n",
            "Q 697+9   T 706  \u001b[92m☑ 706 \u001b[0m\n",
            "Q 776+362 T 1138 \u001b[92m☑ 1138\u001b[0m\n",
            "Q 6+201   T 207  \u001b[92m☑ 207 \u001b[0m\n",
            "Q 607+91  T 698  \u001b[92m☑ 698 \u001b[0m\n",
            "Q 27+977  T 1004 \u001b[92m☑ 1004\u001b[0m\n",
            "Q 496+47  T 543  \u001b[92m☑ 543 \u001b[0m\n",
            "Q 912+3   T 915  \u001b[92m☑ 915 \u001b[0m\n",
            "\n",
            "Iteration 27\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9959 - loss: 0.0152 - val_accuracy: 0.9546 - val_loss: 0.1562\n",
            "Q 20+944  T 964  \u001b[92m☑ 964 \u001b[0m\n",
            "Q 786+56  T 842  \u001b[92m☑ 842 \u001b[0m\n",
            "Q 752+621 T 1373 \u001b[91m☒ 1473\u001b[0m\n",
            "Q 610+507 T 1117 \u001b[92m☑ 1117\u001b[0m\n",
            "Q 41+370  T 411  \u001b[92m☑ 411 \u001b[0m\n",
            "Q 17+163  T 180  \u001b[91m☒ 181 \u001b[0m\n",
            "Q 629+7   T 636  \u001b[92m☑ 636 \u001b[0m\n",
            "Q 6+47    T 53   \u001b[91m☒ 52  \u001b[0m\n",
            "Q 625+44  T 669  \u001b[92m☑ 669 \u001b[0m\n",
            "Q 7+752   T 759  \u001b[91m☒ 769 \u001b[0m\n",
            "\n",
            "Iteration 28\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9929 - loss: 0.0238 - val_accuracy: 0.9995 - val_loss: 0.0029\n",
            "Q 308+54  T 362  \u001b[92m☑ 362 \u001b[0m\n",
            "Q 61+288  T 349  \u001b[92m☑ 349 \u001b[0m\n",
            "Q 601+897 T 1498 \u001b[92m☑ 1498\u001b[0m\n",
            "Q 488+97  T 585  \u001b[92m☑ 585 \u001b[0m\n",
            "Q 1+972   T 973  \u001b[92m☑ 973 \u001b[0m\n",
            "Q 88+88   T 176  \u001b[92m☑ 176 \u001b[0m\n",
            "Q 37+47   T 84   \u001b[92m☑ 84  \u001b[0m\n",
            "Q 27+4    T 31   \u001b[92m☑ 31  \u001b[0m\n",
            "Q 73+764  T 837  \u001b[92m☑ 837 \u001b[0m\n",
            "Q 657+744 T 1401 \u001b[92m☑ 1401\u001b[0m\n",
            "\n",
            "Iteration 29\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9979 - loss: 0.0083 - val_accuracy: 0.9985 - val_loss: 0.0074\n",
            "Q 631+87  T 718  \u001b[92m☑ 718 \u001b[0m\n",
            "Q 56+757  T 813  \u001b[92m☑ 813 \u001b[0m\n",
            "Q 86+19   T 105  \u001b[92m☑ 105 \u001b[0m\n",
            "Q 940+4   T 944  \u001b[92m☑ 944 \u001b[0m\n",
            "Q 429+192 T 621  \u001b[92m☑ 621 \u001b[0m\n",
            "Q 738+5   T 743  \u001b[92m☑ 743 \u001b[0m\n",
            "Q 350+698 T 1048 \u001b[92m☑ 1048\u001b[0m\n",
            "Q 234+312 T 546  \u001b[92m☑ 546 \u001b[0m\n",
            "Q 360+85  T 445  \u001b[92m☑ 445 \u001b[0m\n",
            "Q 5+79    T 84   \u001b[92m☑ 84  \u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NGqR2Qov4akg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n1ZnwLMr4ahs"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}